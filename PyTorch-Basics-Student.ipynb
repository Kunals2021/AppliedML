{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53b188e-a62a-4b80-8c61-41e87f8f72bf",
   "metadata": {},
   "source": [
    "# Homegrown Linear Regression (LR) via numpy\n",
    "\n",
    "Here, we will implement linear regression (a single layer network, with an architecture \"2-1\", corresponding to a model with 2 inputs, and a single output node) using array programming, and also in network fashion using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array class, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; but it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a single-layer network (linear regression) to random data by manually implementing the forward and backward (gradient descent) passes through the network using numpy operations. The following code shows how to accomplish this:\n",
    "\n",
    "\n",
    "The below code trains a linear regression model from random data (so the model learns nothing really; it is just an exercise).\n",
    "\n",
    "\n",
    "**QUESTION** :Run the below code and modify as needed and answer the following question. What is the value of the gradient for the first iteration of gradient descent after running the above code?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec8ced-f056-4c66-82a7-a117fc48b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# m_rows is batch size; \n",
    "# D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 2, 1\n",
    "#m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "np.random.seed(seed=42) #fix the seed\n",
    "# Create random input and output data\n",
    "X_train = np.random.randn(m_rows, D_in)\n",
    "y_train = np.random.randn(m_rows, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "W1 = np.random.randn(D_in, D_out) #[w0, w1, w2, ...w999]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "for epoch in range(10):  #Gradient descent\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = X_train.dot(W1)\n",
    "    #y_pred = X_train @ W1 \n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y_train).mean()\n",
    "    print(f\"Epoch:{epoch}, MSE: {loss}\")\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    err = 2.0 * (y_pred - y_train)\n",
    "    grad_W1 = X_train.T.dot(err)/m_rows  #weighted sum of the train data\n",
    "    print(f\"Epoch:{epoch}, Gradient: {grad_W1}\")\n",
    "    \n",
    "    # Update weights via gradient descent\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    print('--------------------')\n",
    "    \n",
    "print(f\"Weights after training: ,{np.round(W1, 3)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773e621-a5b0-42ec-b7c5-0525c0056196",
   "metadata": {},
   "source": [
    "# Linear regression using tensors and autograd\n",
    "\n",
    "\n",
    "Traditionally, say using Numpy, we had to manually implement both the forward and backward passes to train a  linear regression model (aka a single-layered neural network). Manually implementing the backward pass is not a big deal for a linear regression model or for a small/shallow network, but this can quickly get very tricky for larger multilayers networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality.\n",
    "\n",
    "When using autograd,\n",
    "* **the forward pass of your network (code for making a prediction) will define a computational graph;**\n",
    "* nodes in the graph will be Tensors,\n",
    "* and edges will be functions that produce output Tensors from input Tensors.\n",
    "\n",
    "PyTorch builds up a graph as you compute the forward pass through the network, and one call to **backward()** on some result node (loss function) then augments each intermediate node in the graph with the gradient of the result node with respect to that intermediate node.\n",
    "\n",
    "This sounds complicated, but it’s pretty simple to use in practice. We wrap our PyTorch Tensors in Variable objects; a Variable represents a node in a computational graph. If x is a Variable then x.data is a Tensor, and x.grad is another Variable holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "PyTorch Variables are PyTorch Tensors variables.\n",
    "\n",
    "Here we use PyTorch tensor variables and autograd to implement our single-layer network (linear regression model); now we no longer need to manually implement the backward pass through the network:\n",
    "\n",
    "**Question** Run the below code and modify as needed to get the value of the first element of the learnt linear regression model. What is the value of the first element of the learnt linear regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3960f26-95a9-4650-894c-0379f1c59a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X_train = torch.randn((m_rows, D_in), requires_grad=False) #use (m_rows, D_in) or m_rows, D_in\n",
    "Y_train = torch.randn(m_rows, D_out, requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "W1 = torch.randn(D_in, D_out,  requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for epoch in range(50):\n",
    "    # Forward pass: compute predicted y using operations on Variables; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    Y_pred = X_train.matmul(W1)\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "    # (1,); loss.data[0] is a scalar value holding the loss.\n",
    "    loss = (Y_pred - Y_train).pow(2).mean()\n",
    "    if (epoch +1 )%10 ==0:  #print every 10 epochs\n",
    "        print(f\"Epoch:{epoch+1}, MSE: {loss.data.numpy():.6}\")\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call W1.grad  will be Variables holding the gradient\n",
    "    # of the loss with respect to W1.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent; W1.data and W2.data are Tensors,\n",
    "    # W1.grad are Variables and W1.grad.data aare\n",
    "    # Tensors.\n",
    "    W1.data -= learning_rate * W1.grad.data\n",
    "    # Manually zero the gradients after updating weights\n",
    "    W1.grad.data.zero_()\n",
    "\n",
    "print('------------------------')    \n",
    "# first value of the learnt linear regression model\n",
    "print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a0079-a4e7-4e12-a870-814e10ca9442",
   "metadata": {},
   "source": [
    "# PyTorch: optim\n",
    "\n",
    "\n",
    "Up to this point we have updated the weights of our models by manually mutating the .data member for Variables holding learnable parameters.\n",
    "\n",
    "`W1.data -= learning_rate * W1.grad.data`\n",
    "\n",
    "This is not a huge burden for simple optimization algorithms like stochastic gradient descent on a single layer network (such as a multiple linear regession (MLR) model), but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc. More on these optimizers later.\n",
    "\n",
    "The **optim** package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.\n",
    "\n",
    "The following example brings these two highly scaleable concepts of computational graphs and optimization to life:\n",
    "\n",
    "* we will use the nn package to define our our linear regression module,\n",
    "* and we will use optimize the model using the **Adam** algorithm, a variant of stochastic gradient descent. The **Adam** algorithm comes as part of the the **optim** package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be74e30-04bd-4526-98ba-9504a8406d70",
   "metadata": {},
   "source": [
    "# Building PyTorch Networks using the Sequential API\n",
    "\n",
    "The **Sequential** class allows us to build a neural network in PyTorch in a high-level quick and modular manner.\n",
    "\n",
    "* The Sequential class allows us to build PyTorch neural networks on-the-fly without having to build an explicit class.\n",
    "* This make it much easier to rapidly build networks and allows us to skip over the step where we implement the forward() method. When we use the sequential way of building a PyTorch network, we construct the forward() method implicitly by defining our network's architecture sequentially.\n",
    " \n",
    "\n",
    "Here we use the Sequential() API to build a single-layered neural network (upon closer inspection you see it is a linear regression model) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d3513-7873-4d25-bd5f-83b30b175e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X_train = torch.randn((m_rows, D_in), requires_grad=False) #use (m_rows, D_in) or m_rows, D_in\n",
    "Y_train = torch.randn(m_rows, D_out, requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "# NOT Need as we are using the nn package\n",
    "#  W1 = torch.randn(D_in, D_out,  requires_grad=True)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "# use the sequential API makes things simple\n",
    "model = torch.nn.Sequential(  #  X_train @ W1\n",
    "    torch.nn.Linear(D_in, D_out),   # X.matmul(W1)\n",
    ")\n",
    "# loss scaffolding layer\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Variables it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(50):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, Y_train)\n",
    "    if epoch % 10 == 0:\n",
    "         print(f\"Epoch:{epoch}, MSE: {loss.item():.9}\")\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4281d-4b27-430b-88ae-fdf0a530e14f",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "A single layered  neural network consisting of an input layer (not counted as a layer) and an output layer  is essentially just a linear regression model. A two-layered network consisting for an input layer (not counted as a layer), a hidden layer, and an output layer is still essentially just a linear regression model. Both these networks are just linear transformations of the inputs. To make these networks nonlinear (and enhance their predictive power), we introduce  a non-linear activation function (that acts element-wise on the linear transformed inputs).\n",
    "\n",
    " \n",
    "\n",
    "A popular activation function is the Sigmoid function. It is one of the most widely used non-linear activation function. Sigmoid transforms the values between the range 0 and 1. Here is the mathematical expression for sigmoid:\n",
    "\n",
    "`f(x) = 1/(1+e^-x)`\n",
    "\n",
    "``` import numpy as np\n",
    "def sigmoid_function(x):\n",
    "    z = (1/(1 + np.exp(-x)))\n",
    "    return z\n",
    "```  \n",
    "    \n",
    "**ReLU**\n",
    "\n",
    "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. This means that the neurons will only be deactivated if the output of the linear transformation is less than 0. \n",
    "\n",
    "```\n",
    "def relu_function(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "```\n",
    "\n",
    "`relu_function(7), relu_function(-7). # returns (7, 0) as a result`\n",
    "\n",
    "For the negative part of the input domain, you will notice that the value is zero.\n",
    "\n",
    "In PyTorch ReLU is available as a built in layer via  torch.nn.ReLU().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10745a70-4023-48de-a52b-e67f4ef50e5b",
   "metadata": {},
   "source": [
    "**The following are examples of neural networks consisting of input, hidden, and output layers:**\n",
    "\n",
    "```    \n",
    "network1 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(out_features, 1) # X.matmul(W2) + b2 )\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "network2 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.Linear(out_features, 1) # X.matmul(W2) + b2 )\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "network3 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(in_features, out_features), # X.matmul(W2) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(out_features, 1) # X.matmul(W3) + b2 )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da53625-3312-45fe-ad11-2f75a14c340d",
   "metadata": {},
   "source": [
    "# Boston house price regression via Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a4f2e-324d-43cc-a95f-7505f3fc62f9",
   "metadata": {},
   "source": [
    "Using the starter code below train a neural network (sometimes known as a  multi-layered perceptron) on the Boston house price prediction problem. Experiment with different architectures and report your best one based on the lowest MSE of the test dataset.  Please upload your entire code and the output from your best run here and provide a brief summary and discussion of the best architecture and experimental setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c8e00-fbb0-49f2-baa7-36cdee912f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bb7ff-4e9e-4360-9fbe-ef5571d9f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary  #install it if necessary using !pip install torchsummary \n",
    "import torch\n",
    "#import torchvision\n",
    "import torch.utils.data\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# is there a GPU availabale. If available use it\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(f\"We are working on a {device} device\")\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "X.shape\n",
    "\n",
    "# train validation test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation) #Transform test set with the same constants\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_validation_tensor = torch.from_numpy(X_validation)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "y_validation_tensor = torch.from_numpy(y_validation)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "boston_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "boston_validation = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "boston_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# create dataloader\n",
    "batch_size = 96\n",
    "trainloader_boston = torch.utils.data.DataLoader(boston_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "validloader_boston = torch.utils.data.DataLoader(boston_validation, batch_size=X_test.shape[0], shuffle=False, num_workers=2)\n",
    "testloader_boston = torch.utils.data.DataLoader(boston_test, batch_size=X_validation.shape[0], shuffle=False, num_workers=2)\n",
    "\n",
    "#==================================================#\n",
    "#    Modify the network architecture here          #\n",
    "#==================================================#\n",
    "D_in = X_test.shape[1]\n",
    "#print(D_in)\n",
    "D_hidden =20\n",
    "D_out = 1\n",
    "# Use the nn package to define our model and loss function.\n",
    "# use the sequential API makes things simple\n",
    "model = torch.nn.Sequential(    \n",
    "    torch.nn.Linear(D_in, D_hidden),   # X.matmul(W1)\n",
    "    nn.ReLU(), #nn.Sigmoid()           # Relu( X.matmul(W1))\n",
    "    nn.Linear(in_features=D_hidden, out_features=D_out)       # Relu( X.matmul(W1)).matmul(W2)\n",
    "    \n",
    ")\n",
    "\n",
    "#==================================================#\n",
    "#                                                  #\n",
    "#          Please don't modify code below here     #\n",
    "#==================================================#\n",
    "\n",
    "# MSE loss scaffolding layer\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "print('-'*50)\n",
    "print('Model:')\n",
    "print(model)\n",
    "summary(model, (1, 13))\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "'''\n",
    "Training Process:\n",
    "    Load a batch of data.\n",
    "    Zero the grad.\n",
    "    Predict the batch of the data through net i.e forward pass.\n",
    "    Calculate the loss value by predict value and true value.\n",
    "    Backprop i.e get the gradient with respect to parameters\n",
    "    Update optimizer i.e gradient update\n",
    "'''\n",
    "\n",
    "epochs = range(5)\n",
    "for epoch in epochs:\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for batch, data in enumerate(trainloader_boston):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)\n",
    "\n",
    "\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # do forward pass\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        # compute loss and gradients\n",
    "        loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform gradient update\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss\n",
    "        running_loss += loss.item()\n",
    "        count += 1\n",
    "        \n",
    "    print(f\"Epoch {epoch+1},Train MSE loss: {np.round(running_loss/count, 3)}\")\n",
    "\n",
    "print('Finished Training')\n",
    "print('-'*50)\n",
    "\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for batch, data in enumerate(validloader_boston):\n",
    "    inputs, target = data[0].to(device), data[1].to(device)\n",
    "    # do forward pass\n",
    "    output = model(inputs.float())\n",
    "\n",
    "    # compute loss and gradients\n",
    "    loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    count += 1\n",
    "\n",
    "print(f\" Validation  MSE loss: {np.round(running_loss/count, 3)}\")\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "count = 0\n",
    "for batch, data in enumerate(testloader_boston):\n",
    "    inputs, target = data[0].to(device), data[1].to(device)\n",
    "    # do forward pass\n",
    "    output = model(inputs.float())\n",
    "\n",
    "    # compute loss and gradients\n",
    "    loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    count += 1\n",
    "    \n",
    "print(f\" TEST  MSE loss: {np.round(running_loss/count, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b1a6c-d008-4dab-ae54-f0896b360a73",
   "metadata": {},
   "source": [
    "# Perform Classification on Iris dataset via Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c1042-89f0-4f43-810e-6714220a031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn import datasets\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(f\"We are working on a {device} device\")\n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True)\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "iris_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "iris_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# create dataloader\n",
    "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
    "batch_size = 20   # batch_size is the size of each batch in which data returned \n",
    "trainloader_iris = torch.utils.data.DataLoader(iris_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader_iris = torch.utils.data.DataLoader(iris_test, batch_size=X_test.shape[0], shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "#==================================================#\n",
    "#    Modify the network architecture here          #\n",
    "#==================================================#\n",
    "\n",
    "D_in = X_test.shape[1]\n",
    "#print(D_in)\n",
    "D_hidden = ...\n",
    "D_out = ...    # Hint: The D_out is the number of class\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "# use the sequential API makes things simple\n",
    "model = torch.nn.Sequential(\n",
    "    ...\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# use Cross Entropy and SGD optimizer.\n",
    "loss_fn = ...  #for classfication \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "#==================================================#\n",
    "#                                                  #\n",
    "#          Please don't modify code below here     #\n",
    "#==================================================#\n",
    "\n",
    "#summary(model, (4, 20))\n",
    "print('-'*50)\n",
    "print('Model:')\n",
    "print(model)\n",
    "print('-'*50)\n",
    "\n",
    "epochs = range(5)\n",
    "'''\n",
    "Training Process:\n",
    "    Load a batch of data.\n",
    "    Zero the grad.\n",
    "    Predict the batch of the data through net i.e forward pass.\n",
    "    Calculate the loss value by predict value and true value.\n",
    "    Backprop i.e get the gradient with respect to parameters\n",
    "    Update optimizer i.e gradient update\n",
    "'''\n",
    "train_losses = {}\n",
    "train_accuracy = {}\n",
    "for epoch in epochs:\n",
    "    running_loss = []\n",
    "    y_pred = []\n",
    "    epoch_target = []\n",
    "    for batch, data in enumerate(trainloader_iris):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # do forward pass\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        # compute loss and gradients\n",
    "        loss = loss_fn(output, target)\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform gradient update\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred.extend(torch.argmax(output, dim=1).tolist())\n",
    "        epoch_target.extend(target.tolist())\n",
    "        running_loss.append(loss.item())\n",
    "        \n",
    "    epoch_training_loss = np.mean(running_loss)\n",
    "    train_losses[epoch+1] = epoch_training_loss\n",
    "    print(f\"Epoch {epoch+1}, Training loss: {np.round(epoch_training_loss, 3)}\")\n",
    "    \n",
    "    #accuracy\n",
    "    correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "    accuracy = correct.sum()/ correct.size\n",
    "    train_accuracy[epoch+1] = accuracy\n",
    "    print(f\"Epoch {epoch+1}, Training accuracy: {np.round(accuracy, 3)}\")\n",
    "    \n",
    "print(train_losses)\n",
    "\n",
    "print('Finished Training')\n",
    "print('-'*50)\n",
    "\n",
    "test_batch_losses = []\n",
    "test_y_pred = []\n",
    "test_target = []\n",
    "for batch, data in enumerate(testloader_iris):\n",
    "    inputs, target = data[0].to(device), data[1].to(device)\n",
    "    # do forward pass\n",
    "    output = model(inputs.float())\n",
    "\n",
    "    # compute loss\n",
    "    loss = loss_fn(output, target)\n",
    "    \n",
    "    test_batch_losses.append(loss.item())\n",
    "    \n",
    "    test_y_pred.extend(torch.argmax(output, dim=1).tolist())\n",
    "    test_target.extend(target.tolist())\n",
    "\n",
    "print(f\"Test loss: {np.round(np.mean(test_batch_losses), 3)}\")\n",
    "\n",
    "#accuracy\n",
    "test_correct = (np.array(test_y_pred) == np.array(test_target))\n",
    "test_accuracy = test_correct.sum()/ test_correct.size\n",
    "print(f\"Test accuracy: {np.round(test_accuracy, 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9f920-8f12-4b1b-b701-2f4ce24740a2",
   "metadata": {},
   "source": [
    "# Boston house price regression via OOP API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d2e890-f784-405f-a897-4e661c7d73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are working on a cpu device\n",
      "--------------------------------------------------\n",
      "Model:\n",
      "BaseModel(\n",
      "  (fc1): Linear(in_features=13, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 20]             280\n",
      "            Linear-2                 [-1, 1, 1]              21\n",
      "================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Epoch 1\n",
      "Train MSE loss: 607.063\n",
      "Validation MSE loss: 632.953\n",
      "Epoch 2\n",
      "Train MSE loss: 604.05\n",
      "Validation MSE loss: 630.960\n",
      "Epoch 3\n",
      "Train MSE loss: 605.145\n",
      "Validation MSE loss: 628.982\n",
      "Epoch 4\n",
      "Train MSE loss: 597.789\n",
      "Validation MSE loss: 627.023\n",
      "Epoch 5\n",
      "Train MSE loss: 597.191\n",
      "Validation MSE loss: 625.078\n",
      "--------------------------------------------------\n",
      "Test MSE loss: 470.058\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary  #install it if necessary using !pip install torchsummary \n",
    "import torch\n",
    "#import torchvision\n",
    "import torch.utils.data\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "    \n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "\n",
    "# train validation test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train =      scaler.fit_transform(X_train).astype(float)\n",
    "X_validation = scaler.transform(X_validation).astype(float) #Transform valid set with the same constants\n",
    "X_test =       scaler.transform(X_test).astype(float)       #Transform test  set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_validation_tensor = torch.from_numpy(X_validation).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()\n",
    "y_validation_tensor = torch.from_numpy(y_validation).float()\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "train_ds = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "validation_ds = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "test_ds = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# create dataloader\n",
    "batch_size = 96\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(validation_ds, batch_size=X_test.shape[0], shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=X_validation.shape[0], shuffle=False, num_workers=0)\n",
    "\n",
    "#==================================================#\n",
    "#    Modify the network architecture here          #\n",
    "#==================================================#\n",
    "\n",
    "D_in = X_test.shape[1]\n",
    "#print(D_in)\n",
    "D_hidden =20\n",
    "D_out = 1\n",
    "\n",
    "# Use the OOP API to define a deep neural network model\n",
    "#\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"Custom module for a simple  regressor\"\"\"\n",
    "    def __init__(self, in_features, size_hidden=10, n_output=1):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features, size_hidden)   # hidden layer\n",
    "        self.fc2 = torch.nn.Linear(size_hidden, n_output)      # output layer\n",
    " \n",
    "    def forward(self, x):\n",
    "   \n",
    "        x = F.relu(self.fc1(x))   # activation function for hidden layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(f\"We are working on a {device} device\")\n",
    "\n",
    "# create classifier and optimizer objects\n",
    "model = BaseModel(in_features=D_in, size_hidden = D_hidden, n_output = D_out)\n",
    "model.to(device) # put on GPU before setting up the optimizer\n",
    "\n",
    "#==================================================#\n",
    "#                                                  #\n",
    "#          Please don't modify code below here     #\n",
    "#==================================================#\n",
    "\n",
    "print('-'*50)\n",
    "print('Model:')\n",
    "print(model)\n",
    "summary(model, (1, 13))\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "opt = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "'''\n",
    "Training Process:\n",
    "    Load a batch of data.\n",
    "    Zero the grad.\n",
    "    Predict the batch of the data through net i.e forward pass.\n",
    "    Calculate the loss value by predict value and true value.\n",
    "    Backprop i.e get the gradient with respect to parameters\n",
    "    Update optimizer i.e gradient update\n",
    "'''\n",
    "\n",
    "def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    # dataset API gives us pythonic batching \n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)        \n",
    "        # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "        opt.zero_grad()\n",
    "        preds = model(inputs) #prediction over the input data\n",
    "\n",
    "        # compute loss and gradients\n",
    "        loss = loss_fn(preds, target)    #mean loss for this batch\n",
    "\n",
    "        loss.backward() #calculate nabla_w\n",
    "        loss_history.append(loss.item())\n",
    "        opt.step()  #update W\n",
    "        #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "    print(f\"Train MSE loss: {np.round(running_loss/count, 3)}\")\n",
    "\n",
    "\n",
    "\n",
    "#from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "    overall_loss = 0.0\n",
    "    count = 0\n",
    "    for i,data in enumerate(data_loader):\n",
    "        inputs, targets = data[0].to(device), data[1].to(device)                \n",
    "        outputs = model(inputs)      \n",
    "\n",
    "        loss = loss_fn(outputs, targets)           # compute loss value\n",
    "        \n",
    "        overall_loss += (loss.item())  # compute total loss to save to logs\n",
    "        count += 1\n",
    "        \n",
    "        # compute mean loss\n",
    "\n",
    "    print(f\"{tag} MSE loss: {overall_loss/count:.3f}\")\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    train_epoch(epoch, model, loss_fn, opt, train_loader)\n",
    "    evaluate_model(epoch,    model, loss_fn, opt, valid_loader, tag = \"Validation\")\n",
    "print(\"-\"*50)\n",
    "evaluate_model(epoch, model, loss_fn, opt, test_loader, tag=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d6090-48dd-4647-b232-0e2c10d473d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
